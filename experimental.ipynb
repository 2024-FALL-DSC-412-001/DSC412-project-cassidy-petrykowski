{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import cv2 as cv2\n",
    "from PIL import Image\n",
    "\n",
    "#The line below is necessary to show Matplotlib's plots inside a Jupyter Notebook\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cassidy/DSC412/project/DSC412-project-cassidy-petrykowski/data/grids/cf43b502b7/393c2b73c6.png\n",
      "/home/cassidy/DSC412/project/DSC412-project-cassidy-petrykowski/data/grids/e0ad521fa7/3a14792b4a.png\n",
      "/home/cassidy/DSC412/project/DSC412-project-cassidy-petrykowski/data/grids/7eac066d8e/4eef4e6b06.png\n",
      "/home/cassidy/DSC412/project/DSC412-project-cassidy-petrykowski/data/grids/2ca99a36b4/4eef4e6b06.png\n",
      "/home/cassidy/DSC412/project/DSC412-project-cassidy-petrykowski/data/grids/25aa115aec/4eef4e6b06.png\n",
      "/home/cassidy/DSC412/project/DSC412-project-cassidy-petrykowski/data/grids/6f0795f9b5/af908319c4.png\n",
      "Width: 218 Height: 218\n"
     ]
    }
   ],
   "source": [
    "# read csv into dataframe\n",
    "data = pd.read_csv(\"data/patterns.csv\")\n",
    "storage_path = \"/home/cassidy/DSC412/project/DSC412-project-cassidy-petrykowski/data\"\n",
    "\n",
    "# inspect the shape\n",
    "# print(\"Data Shape: \" + str(data.shape))\n",
    "\n",
    "# get the average image size\n",
    "width, height = get_average_image_size('/home/cassidy/DSC412/project/DSC412-project-cassidy-petrykowski/data/grids')\n",
    "print(\"Width: \" + str(width), \"Height: \" + str(height))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 1: Isolate the primary object in the image and fill in the backdrop\n",
    "\n",
    "This uses an image segmentation technique called thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@1795.536] global loadsave.cpp:241 findDecoder imread_('/home/cassidy/DSC412/project/DSC412-project-cassidy-petrykowski/data/photos/da23962d90/8ec0b4ecf9.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(img_path)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#Use this helper function if you are working in Jupyter Lab\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#If not, then directly use cv2.imshow(<window name>, <image>)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[43mbgremove2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m showimage(img)\n\u001b[1;32m     12\u001b[0m merged_path \u001b[38;5;241m=\u001b[39m merge_images(img_path,img_path, (storage_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/merged/\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/DSC412/project/DSC412-project-cassidy-petrykowski/helper_functions.py:38\u001b[0m, in \u001b[0;36mbgremove2\u001b[0;34m(myimage)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbgremove2\u001b[39m(myimage):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# First Convert to Grayscale\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     myimage_grey \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmyimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     ret,baseline \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mthreshold(myimage_grey,\u001b[38;5;241m127\u001b[39m,\u001b[38;5;241m255\u001b[39m,cv2\u001b[38;5;241m.\u001b[39mTHRESH_TRUNC)\n\u001b[1;32m     42\u001b[0m     ret,background \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mthreshold(baseline,\u001b[38;5;241m126\u001b[39m,\u001b[38;5;241m255\u001b[39m,cv2\u001b[38;5;241m.\u001b[39mTHRESH_BINARY)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "#  Thresholding from open cv \n",
    "\n",
    "# Read image\n",
    "img_path = storage_path + 'cxfdfdff'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "#Use this helper function if you are working in Jupyter Lab\n",
    "#If not, then directly use cv2.imshow(<window name>, <image>)\n",
    " \n",
    "b = bgremove2(img)\n",
    "showimage(img)\n",
    "merged_path = merge_images(img_path,img_path, (storage_path + \"/merged/\"))\n",
    "merged = cv2.imread(merged_path)\n",
    "\n",
    "\n",
    "# will update so that images are adjusted when they are imported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Split the data between training and testing\n",
    "\n",
    "Because the csv contains the names for both the photos and grid, it can be split into one training set and one testing set.\n",
    "\n",
    "When the data is actually accessed, the distinction between inputs and outputs or photos and grids will be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    7092f1985d\n",
      "43  edba2a6bbe\n",
      "39  09dbfb528d\n",
      "35  51684b0d4c\n",
      "23  c3cbf850c5\n",
      "45  b1b55738b1\n",
      "10  2666e3c94b\n",
      "22  ef642b8b75\n",
      "18  c3af8775ca\n",
      "55  f9232fe1b3\n",
      "20  95fa1a221a\n",
      "7   113fbb67d4\n",
      "42  2e024bf09e\n",
      "14  1e1bdfb2e7\n",
      "28  df03231a62\n",
      "51  337c0070cc\n",
      "38  7ce54e685d\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = train_test_split(             data,\n",
    "                                                    test_size=0.7, \n",
    "                                                    random_state=42\n",
    "                                                   )\n",
    "print(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
